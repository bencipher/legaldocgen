import asyncio
import random
from typing import Dict, List, Optional, Any, AsyncGenerator

MAX_FIELD_TO_REQUEST = random.randint(1, 2)
USER_INPUT_OBJ_HISTORY = set()


class MockLLM:
    """A mock LLM/agent to simulate LLM calls (replace with real model later)."""

    async def extract_requirements(self, user_prompt: str) -> List[str]:
        await asyncio.sleep(1)
        # In reality, the LLM will extract fields like "issuer_name, receiver_name..."
        if "purchase" in user_prompt.lower():
            return ["issuer_name", "receiver_name", "date", "location", "item_description"]
        elif "rental" in user_prompt.lower():
            return ["landlord_name", "tenant_name", "property_address", "rental_period", "amount"]
        else:
            return ["party_a", "party_b", "subject", "date"]

    async def ask_for_field(self, missing_fields: List[str], user_last_action: str) -> str:
        """Ask the user for one or two fields at a time."""
        await asyncio.sleep(0.5)
        # to the final prompt to be generated by the agent, the agent must acknowledge or thank the user in case they saved a field recently, and then proceed to ask for the other info
        subset = missing_fields[:MAX_FIELD_TO_REQUEST]
        joined = " and ".join(subset)
        # llm call to be made here (which acks user past input and subtly ask for further ones)
        built_in_acknowledgment = ""
        if user_last_action:
            built_in_acknowledgment = "The user just performed the following actions: " + user_last_action + ". "

        return f"""The user wants to generate a document. Here are the required fields to
                needed to ensure the documents is properly generated: {built_in_acknowledgment}.
                The following information is still needed: {joined}.
                Craft a polite and concise question to ask the user for {joined}, acknowledging or thanking them for any recent inputs they provided."""

    async def thank_user(self, field_name: str, value: str) -> None:
        await asyncio.sleep(0.2)
        print(f"Thanks! I've recorded {field_name} as '{value}'.")
        USER_INPUT_OBJ_HISTORY.add(f"{field_name} as '{value}'.")

    async def generate_document(self, context: Dict[str, Any]) -> AsyncGenerator[str, None]:
        """Stream document chunks (mock)."""
        doc_text = f"Legal Document Generated for {context.get('issuer_name', 'Unknown')}.\n" f"Details:\n{context}"
        for chunk in doc_text.split():
            await asyncio.sleep(0.2)
            yield chunk + " "

    async def save_fields(self, user_input: str, missing: List[str], fields: Dict[str, Any]) -> None:
        await asyncio.sleep(0.3)
        # In reality, the LLM would parse and save fields from user input

        print(f"(LLM saved fields from user input: '{user_input}')")
        for field in missing:
            print(f" - {field}: {user_input}")
            fields[field] = user_input


class DocumentOrchestrator:
    """
    Orchestrates a multi-phase conversational flow:
    - Phase 1: Requirement extraction
    - Phase 2: Interactive requirement filling
    - Phase 3: Document generation
    """

    def __init__(self, llm: Optional[MockLLM] = None):
        self.llm = llm or MockLLM()
        self.fields: Dict[str, Optional[str]] = {}
        self.state = "idle"

    async def start(self, user_prompt: str):
        """Start flow: extract requirements."""
        self.state = "extracting"
        print(f"Extracting requirements for: '{user_prompt}'")
        field_list = await self.llm.extract_requirements(user_prompt)
        self.fields = {f: None for f in field_list}
        self.state = "collecting"

    def _missing_fields(self) -> List[str]:
        return [k for k, v in self.fields.items() if not v]

    async def next_question(self) -> Optional[str]:
        """Use llm to ask user for the next missing fields while acknowledging last actions."""
        missing = self._missing_fields()
        if not missing:
            self.state = "generating"
            return None
        user_last_action = ", ".join([action for action in USER_INPUT_OBJ_HISTORY])
        USER_INPUT_OBJ_HISTORY.clear()
        return await self.llm.ask_for_field(missing, user_last_action)

    async def record_user_input(self, user_response: str):
        """Fill next missing field(s) based on user input (simple mock logic)."""
        self._ensure_state("collecting", "Cannot record user input before starting collection.")
        missing = self._missing_fields()
        if not missing:
            return

        # Simple mapping: assign sequentially
        # use LLM to map out the fields from the user input so it can save responses in sentence form
        for field in missing[:MAX_FIELD_TO_REQUEST]:
            self.fields[field] = user_response

            await self.llm.thank_user(field, user_response)  # record just gotten user input for ack
        await self.llm.save_fields(user_response, missing, self.fields)  # let llm save the fields from user input

    async def generate_document(self):
        """Run the document generation phase (streaming chunks)."""
        self._ensure_state("generating", "Cannot generate document before collecting all fields.")

        print("AIGenerating document...")
        async for chunk in self.llm.generate_document(self.fields):
            yield chunk

    async def get_user_goal(self, initial_msg: str) -> str:
        """Confirm or clarify user intent before extraction."""
        await asyncio.sleep(0.5)
        # Mock confirmation (in a real model, you'd use LLM classification)
        if "agreement" in initial_msg.lower():
            goal = "Generate a legal agreement document"
        elif "invoice" in initial_msg.lower():
            goal = "Generate an invoice document"
        else:
            goal = "Generate a general-purpose document"
        print(f"Understood your goal: {goal}")
        return goal

    def _ensure_state(self, expected: str, error_msg: str = ""):
        if not error_msg:
            error_msg = f"Invalid state: expected {expected}, got {self.state}"
        if self.state != expected:
            raise RuntimeError(error_msg)


# === DEMO USAGE ===
async def main():
    orchestrator = DocumentOrchestrator()
    start_prompt = input("AI: What doc would you like me to generate today ?: \n")
    await orchestrator.start(start_prompt)
    # await orchestrator.record_user_input(start_prompt) # EXTRACT POSSIBLE FIELDS FROM FIRST MESSAGE
    user_goal = await orchestrator.get_user_goal(start_prompt)
    # Phase 2: Gather missing info
    while orchestrator._missing_fields():
        question = await orchestrator.next_question()
        if not question:
            break
        print()
        user_input = input(f"AI: {question}:\n")
        await orchestrator.record_user_input(user_input)

    # Phase 3: Stream generation
    async for chunk in orchestrator.generate_document():
        print(chunk, end="", flush=True)
    print("\nDone.")


if __name__ == "__main__":
    asyncio.run(main())
